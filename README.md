## **Project** : Mobile Agent For Smart Home Using Multimodal Learning <2023-11-13>
### **Project Field** : Multimodal Learning, Sound Classification, Image captioning, machine traslation, transformer

#### **Project Description** : ~~The project involves developing a multimodal learning model that combines image and sound datasets to analyze specific events, such as detecting a fall in elderly people. The process of developing such a model includes the following steps~~: <br>프로젝트 초기 Background Research 부족으로 다음과 같이 내용을 수정합니다:
Google의 LaMDA나 유명한 OpenAI의 Chatgpt와 같은 언어 모델의 등장으로 텍스트 생성, 기계번역, 질문 응답 시스템과 같은 분야에서 각각의 모델들은 큰 기여를 해왔습니다. 그리고 Vision-Language Models과 같이 두 모달리티 사이의 상호 작용을 이해하고 활용하는 CLIP, DALL-E와 같은 모델들의 등장으로 Multimodality의 시대가 도래했다. Vision과 text뿐만 아니라 다양한 Modalities의 활용은 기존의 하나의 Modality를 가진 모델보다 풍부하고 다양한 작업을 수행할 수 있다. 이 프로젝트에서는 독거노인을 위한 위급상황 탐지가 가능한 로봇에 탑제되는 Multimodal Model을 개발하는 것을 목표로 한다. Audio classification을 통한 이진분류를 통해 Scream / Non-Scream를 통해 상황을 포착하고, Image captioning 기술을 통해 그 가족이나 응급시설에 빠르게 연락을 취할 수 있는 Model을 개발하려합니다.

### **Progress of Research** : 
1. Text Generation
2. seq2seq Model & Attention
3. Transformer
4. Image Captioning







